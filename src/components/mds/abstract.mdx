# Abstract

Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to the self- attention mechanism, which requires a token to consider all preceding tokens as its context to com- pute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the con- text tends to increase. The redundant context not only hampers the modeling representation perfor- mance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Atten- tion for efficient long-context modeling, compris- ing two complementary modules: 1) Globality- aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strength- ens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality- preserving module incorporates neighboring to- kens to preserve local context for detailed repre- sentation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Our CCA-Attention reduces computational complexity to linear scale, achieves **7.9Ã—** speedup and **93%** KV cache memory reduction on 128K context, and outperforms state-of-the-art methods on long-context benchmarks.