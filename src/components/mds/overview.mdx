# Method Overview (CCA-Attention)

We propose **Core Context Aware Attention (CCA-Attention)**, a plug-and-play module for efficient long-context modeling in LLMs. By dynamically compressing redundant tokens into core representations and preserving local details, CCA-Attention achieves **7.9Ã—** speedup on 128K-length contexts while maintaining accuracy.

![img](./imgs/visual.png)
*Figure: Visualization of core vs. redundant context attention scores in LLaMA2-7B*
