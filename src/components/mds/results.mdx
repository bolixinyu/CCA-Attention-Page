# Results
## 1. Long Context Modeling Performance  
Outperforms StreamingLLM, LM-Infinite, and MInference on average scores

![img](./imgs/longbench.png)
*Table 1: Performance on LongBench-E (higher is better)*

## 2. Computational Efficiency  
![Figure 4](./imgs/efficiency.png)  
*Figure 4: Latency and memory comparison with SOTA methods*

## 3. Inference Flexibility
Adjustable group size $g$ and window size $s$ enable trade-offs between speed and accuracy.

![Figure 3](./imgs/flexity.png)  
*Figure 3: Trade-off between latency and accuracy via adjustable $g$ and $s$*